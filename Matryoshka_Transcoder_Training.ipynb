{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Matryoshka Transcoder Training (Nested Groups)\n",
        "\n",
        "This notebook trains a Matryoshka Transcoder with **nested groups** architecture on Google Colab.\n",
        "\n",
        "## üéØ What You'll Get\n",
        "- **Nested Groups**: Each group includes all previous groups (matches original paper)\n",
        "- **Hierarchical Learning**: Features organize from coarse to fine\n",
        "- **Adaptive Complexity**: Each group is a complete model\n",
        "- **Interpretability**: Activation samples for feature analysis\n",
        "\n",
        "## üìä Training Options\n",
        "- **Layer 17** (Recommended): ~6 hours, research quality\n",
        "- **Layer 8** (Quick Test): ~30 minutes, proof of concept\n",
        "- **Layer 12** (Alternative): ~8 hours, mid-layer analysis\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup & Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(\"üñ•Ô∏è  GPU Check:\")\n",
        "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
        "\n",
        "print(\"\\nüì¶ Installing dependencies...\")\n",
        "%pip install torch transformers transformer-lens wandb datasets einops jaxtyping -q\n",
        "print(\"‚úÖ Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository with nested groups implementation\n",
        "print(\"üì• Cloning repository...\")\n",
        "!git clone https://github.com/a1842806/matryoshka_transcoder.git\n",
        "%cd matryoshka_transcoder\n",
        "\n",
        "# Switch to the nested groups branch\n",
        "!git checkout interpretability-evaluation\n",
        "!git pull\n",
        "print(\"‚úÖ Repository cloned and updated\")\n",
        "\n",
        "# Verify we're on the right branch\n",
        "!git branch --show-current\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîê Weights & Biases Setup (Optional)\n",
        "\n",
        "W&B tracks your training metrics. You can skip this if you don't want to use it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to Weights & Biases (optional)\n",
        "import wandb\n",
        "print(\"üîê Weights & Biases Login\")\n",
        "print(\"   This will open a browser tab for authentication\")\n",
        "print(\"   You can skip this by interrupting the cell (Ctrl+C)\")\n",
        "\n",
        "try:\n",
        "    wandb.login()\n",
        "    print(\"‚úÖ W&B login successful\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  W&B login skipped - metrics won't be tracked online\")\n",
        "    print(\"   Training will still work, just without W&B dashboard\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Choose Training Configuration\n",
        "\n",
        "Select one of the training options below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: Layer 17 (Recommended) - ~6 hours on T4 GPU\n",
        "\n",
        "**Best for**: Research quality results, final training\n",
        "- Dictionary size: 18,432 features (nested groups)\n",
        "- Training steps: ~15,000\n",
        "- Features: Activation sample collection\n",
        "- Optimized hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on Layer 17 (RECOMMENDED)\n",
        "print(\"üöÄ Starting Layer 17 training...\")\n",
        "print(\"   This will take ~6 hours on T4 GPU\")\n",
        "print(\"   You can monitor progress on W&B dashboard\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "!python src/scripts/train_gemma_layer17_with_warmup_decay_samples.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Layer 8 (Quick Test) - ~30 minutes on T4 GPU\n",
        "\n",
        "**Best for**: Testing setup, quick experiments\n",
        "- Dictionary size: 18,432 features (nested groups)\n",
        "- Training steps: ~1,000\n",
        "- Fast iteration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on Layer 8 (QUICK TEST)\n",
        "print(\"üöÄ Starting Layer 8 training...\")\n",
        "print(\"   This will take ~30 minutes on T4 GPU\")\n",
        "print(\"   Good for testing your setup\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "!python src/scripts/train_gemma_layer8_with_warmup_decay_samples.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 3: Layer 12 (Alternative) - ~8 hours on T4 GPU\n",
        "\n",
        "**Best for**: Mid-layer analysis, comprehensive training\n",
        "- Dictionary size: 36,864 features (nested groups)\n",
        "- Training steps: ~20,000\n",
        "- Full training run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on Layer 12 (ALTERNATIVE)\n",
        "print(\"üöÄ Starting Layer 12 training...\")\n",
        "print(\"   This will take ~8 hours on T4 GPU\")\n",
        "print(\"   Comprehensive training run\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "!python src/scripts/train_gemma_layer12_with_warmup_decay_samples.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Save Results to Google Drive\n",
        "\n",
        "Run this after training completes to save your checkpoints to Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and save results\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"üíæ Saving results to Google Drive...\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "src_checkpoints = 'checkpoints/transcoder/gemma-2-2b/'\n",
        "dst_checkpoints = '/content/drive/MyDrive/matryoshka_checkpoints/'\n",
        "\n",
        "# Check if checkpoints exist\n",
        "if os.path.exists(src_checkpoints):\n",
        "    # Create destination directory\n",
        "    os.makedirs(dst_checkpoints, exist_ok=True)\n",
        "    \n",
        "    # Copy checkpoints\n",
        "    shutil.copytree(src_checkpoints, dst_checkpoints, dirs_exist_ok=True)\n",
        "    \n",
        "    print(f\"‚úÖ Checkpoints saved to: {dst_checkpoints}\")\n",
        "    \n",
        "    # List saved files\n",
        "    print(\"\\nüìÅ Saved files:\")\n",
        "    for root, dirs, files in os.walk(dst_checkpoints):\n",
        "        for file in files:\n",
        "            if file.endswith(('.pt', '.json')):\n",
        "                rel_path = os.path.relpath(os.path.join(root, file), dst_checkpoints)\n",
        "                print(f\"   - {rel_path}\")\n",
        "    \n",
        "    # Also save activation samples if they exist\n",
        "    activation_samples_dirs = [d for d in os.listdir(src_checkpoints) if 'activation_samples' in d]\n",
        "    if activation_samples_dirs:\n",
        "        dst_samples = '/content/drive/MyDrive/matryoshka_samples/'\n",
        "        os.makedirs(dst_samples, exist_ok=True)\n",
        "        \n",
        "        for sample_dir in activation_samples_dirs:\n",
        "            src_sample = os.path.join(src_checkpoints, sample_dir)\n",
        "            dst_sample = os.path.join(dst_samples, sample_dir)\n",
        "            shutil.copytree(src_sample, dst_sample, dirs_exist_ok=True)\n",
        "        \n",
        "        print(f\"‚úÖ Activation samples saved to: {dst_samples}\")\n",
        "    \n",
        "    print(\"\\nüéâ All results saved to Google Drive!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No checkpoints found. Make sure training completed successfully.\")\n",
        "    print(f\"   Expected location: {src_checkpoints}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Next Steps\n",
        "\n",
        "### What You've Accomplished:\n",
        "‚úÖ Trained a Matryoshka Transcoder with **nested groups**  \n",
        "‚úÖ Implemented hierarchical feature learning  \n",
        "‚úÖ Collected activation samples for interpretability  \n",
        "‚úÖ Saved results to Google Drive  \n",
        "‚úÖ Evaluated model performance  \n",
        "\n",
        "### Your Files:\n",
        "- **Checkpoints**: `/content/drive/MyDrive/matryoshka_checkpoints/`\n",
        "- **Activation Samples**: `/content/drive/MyDrive/matryoshka_samples/`\n",
        "- **Evaluation Results**: In your model's `interpretability_eval/` folder\n",
        "\n",
        "### Further Analysis:\n",
        "1. **Compare with Google's Transcoder**: Use `src/eval/compare_interpretability.py`\n",
        "2. **Feature Visualization**: Analyze the activation samples in detail\n",
        "3. **Hierarchical Analysis**: Study how different groups learn different abstractions\n",
        "4. **Transfer Learning**: Use your transcoder for downstream tasks\n",
        "\n",
        "### Documentation:\n",
        "- **Quick Reference**: `QUICK_START_COLAB.md`\n",
        "- **Detailed Guide**: `COLAB_TRAINING.md`\n",
        "- **Architecture**: `NESTED_GROUPS_SUMMARY.md`\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Understanding Nested Groups\n",
        "\n",
        "Your transcoder now uses **nested groups** (matching the original Matryoshka paper):\n",
        "\n",
        "| Group | Features | Description |\n",
        "|-------|----------|-------------|\n",
        "| 0 | 0-1151 | Base coarse features |\n",
        "| 1 | 0-3455 | Includes Group 0 + new features |\n",
        "| 2 | 0-8063 | Includes Groups 0,1 + new features |\n",
        "| 3 | 0-18431 | All features (complete model) |\n",
        "\n",
        "**Benefits:**\n",
        "- üéØ **Hierarchical Learning**: Features organize coarse ‚Üí fine\n",
        "- üîÑ **Adaptive Complexity**: Each group is a complete model\n",
        "- üìö **Paper Alignment**: Matches original Matryoshka design\n",
        "- üß† **Better Interpretability**: Clear feature hierarchy\n",
        "\n",
        "**Happy training! üöÄ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
