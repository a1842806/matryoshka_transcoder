<!-- 01cc9464-1b4c-4920-b8b3-2481744322ba ed054ddf-63bb-438a-a330-3fd4c24d6a9c -->
# Integrate SAE Bench: compare L17 first, then L12 (Gemma 2 2B)

### Phase 0) Quick comparison at Layer 17 (use existing artifact)

- Existing artifacts detected:
  - Weights: `checkpoints/gemma-2-2b_blocks.17.hook_resid_pre_4608_topk_48_0.0003_*/{config.json, sae.pt}`
  - Activation samples: `checkpoints/transcoder/gemma-2-2b/gemma-2-2b_blocks.17.hook_resid_pre_4608_topk_48_0.0003_activation_samples/…`
- Config shows mapping `resid_mid → mlp_out` at layer 17 (`source_hook_point=blocks.17.hook_resid_mid`, `target_hook_point=blocks.17.hook_mlp_out`), `dict_size=4608`, `top_k=48`.
- Google transcoders map `mlp_in → mlp_out`. Inputs differ; results are still informative but not strictly apples-to-apples. We’ll note this caveat in the comparison report.

Steps

1. Implement SAE Bench adapters:

   - `OurL17TranscoderAdapter`: consumes `resid_mid` and reconstructs `mlp_out`.
   - `GoogleL17TranscoderAdapter`: consumes `mlp_in` and reconstructs `mlp_out`.

2. Create `configs/saebench/gemma2b_l17.yaml` with:

   - Model id `google/gemma-2-2b`, tokenizer, device, batch size
   - Hook points: ours (`blocks.17.hook_resid_mid` → `blocks.17.hook_mlp_out`), Google (`blocks.17.hook_mlp_in` → `blocks.17.hook_mlp_out`)
   - Paths to checkpoints and (optional) activation samples directory
   - Eval dataset path; same split for both

3. Evaluate metrics with SAE Bench:

   - Auto interpretation score, absorption rate; plus reconstruction MSE and active feature rate
   - Save under `analysis_results/sae_bench/gemma-2-2b/l17/{ours|google}/metrics.json`

4. Compare results:

   - Produce side-by-side `comparison.csv` and `comparison.json` at `analysis_results/sae_bench/gemma-2-2b/l17/`

CLI examples

```bash
# Evaluate ours
python -m src.eval.eval_saebench --config configs/saebench/gemma2b_l17.yaml --which ours
# Evaluate Google
python -m src.eval.eval_saebench --config configs/saebench/gemma2b_l17.yaml --which google
# Compare
python -m src.eval.compare_transcoders \
  --ours analysis_results/sae_bench/gemma-2-2b/l17/ours/metrics.json \
  --google analysis_results/sae_bench/gemma-2-2b/l17/google/metrics.json
```

### Phase 1) L12 apples-to-apples (mlp_in → mlp_out)

- Train new L12 transcoder mapping `mlp_in → mlp_out`, save per-feature top activation examples.
- Add SAE Bench adapters and config `configs/saebench/gemma2b_l12.yaml`.
- Run metrics for ours and Google’s L12, then compare.

### Notes & alignment checks

- L17 comparison uses different inputs (`resid_mid` vs `mlp_in`); hold dataset, batch size, dtype, and random seeds constant to reduce confounds.
- L12 phase restores a strict apples-to-apples comparison with identical input/target mapping.

### To-dos

- [ ] Confirm L17 weights and activation samples paths
- [ ] Create SAE Bench adapters for our L17 and Google L17
- [ ] Add configs/saebench/gemma2b_l17.yaml
- [ ] Run SAE Bench metrics for our L17 transcoder
- [ ] Run SAE Bench metrics for Google L17 transcoder
- [ ] Aggregate L17 metrics and output CSV/JSON comparison
- [ ] Train our L12 transcoder; save per-feature top activation examples
- [ ] Add SAE Bench adapters for our and Google’s L12 transcoders
- [ ] Add configs/saebench/gemma2b_l12.yaml
- [ ] Run SAE Bench metrics for our L12 transcoder
- [ ] Run SAE Bench metrics for Google’s L12 transcoder
- [ ] Aggregate L12 metrics and output CSV/JSON comparison